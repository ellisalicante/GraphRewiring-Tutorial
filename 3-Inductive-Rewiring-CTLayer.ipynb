{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d07b1b0",
      "metadata": {
        "id": "6d07b1b0"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ellisalicante/GraphRewiring-Tutorial/blob/main/3-Inductive-Rewiring-CTLayer.ipynb)\n",
        "# Inductive rewiring using CT-Layer\n",
        "***Tutorial on Graph Rewiring: From Theory to Applications in Fairness***\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ellisalicante/GraphRewiring-Tutorial/blob/main/3-Inductive-Rewiring-CTLayer.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "090e809d",
      "metadata": {
        "id": "090e809d"
      },
      "outputs": [],
      "source": [
        "COLLAB_ENV = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "34c457a3",
      "metadata": {
        "id": "34c457a3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f2befe8a",
      "metadata": {
        "id": "f2befe8a",
        "outputId": "7e267c25-19c6-4d25-88f1-1ceaaddc2926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GraphRewiring-Tutorial'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 29 (delta 10), reused 20 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n",
            "Submodule 'DiffWire' (https://github.com/AdrianArnaiz/DiffWire/) registered for path 'DiffWire'\n",
            "Cloning into '/content/GraphRewiring-Tutorial/DiffWire'...\n",
            "Submodule path 'DiffWire': checked out '5aded812451639187680c2d70bf16f4d2f21ca2e'\n",
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 6.6 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "if COLLAB_ENV:\n",
        "    !git clone https://github.com/ellisalicante/GraphRewiring-Tutorial\n",
        "    !cd GraphRewiring-Tutorial && git submodule update --init --recursive\n",
        "    !mv GraphRewiring-Tutorial/* ./\n",
        "    !rm -rf GraphRewiring-Tutorial\n",
        "    \n",
        "    os.environ['TORCH'] = torch.__version__\n",
        "    print(torch.__version__)\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./DiffWire\")"
      ],
      "metadata": {
        "id": "MpafaIKspIgO"
      },
      "id": "MpafaIKspIgO",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from DiffWire.transforms import FeatureDegree\n",
        "from DiffWire.layers.utils.ein_utils import _rank3_diag, _rank3_trace\n",
        "from DiffWire.layers.MinCut_Layer import dense_mincut_pool\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.nn import DenseGraphConv\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "uoZK6-FgiUar"
      },
      "id": "uoZK6-FgiUar",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d16edf02",
      "metadata": {
        "id": "d16edf02"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "--DDi3s5kpOr",
        "outputId": "a065763f-e6a6-4e43-9bd6-8f2bf580bbbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "--DDi3s5kpOr",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "67ccdbfc",
      "metadata": {
        "id": "67ccdbfc"
      },
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yfrlFryzmQPj",
        "outputId": "0c81636a-70b9-44f5-fa07-3792eda3ecf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yfrlFryzmQPj",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  7 12:52:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    30W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b3dde99",
      "metadata": {
        "id": "0b3dde99"
      },
      "source": [
        "## Graph Classification with CT-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa86d2bc",
      "metadata": {
        "id": "aa86d2bc"
      },
      "source": [
        "### Train with CT-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bcbd767",
      "metadata": {
        "id": "4bcbd767"
      },
      "source": [
        "For using it straightforward from the **[DiffWire repository](https://github.com/AdrianArnaiz/DiffWire)**:\n",
        "```python\n",
        "from DiffWire.layers.CT_layer import dense_CT_rewiring\n",
        "```\n",
        "**However, for the sake of clarity of the tutorial, we will explain the content of that function line by line**\n",
        "\n",
        "<a href =\"https://paperswithcode.com/method/ct-layer\"> <img src=\"https://production-media.paperswithcode.com/methods/305a898a-e0a2-4d74-b8e8-c12839496577.png\" alt=\"CT Layer\" style=\"width:500px;\"/> </a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "734c29cf",
      "metadata": {
        "cellView": "form",
        "id": "734c29cf"
      },
      "outputs": [],
      "source": [
        "#@title CT-Layer\n",
        "def dense_CT_rewiring(x, adj, s, mask=None, EPS=1e-15):\n",
        "    \"\"\"Rewires a Graph using CT Distance (Effective Resistances) given 's' as the CT Embedding. \n",
        "    Returns the new adjacency, and the loss for the CT Embbeding (s).\n",
        "\n",
        "    Args:\n",
        "        x (dense): feature matrix: NxF\n",
        "        adj (dense): dense adjacency matrix: NxN\n",
        "        s (dense): CT Embedding: NxH (H: size of latent space)\n",
        "        mask (dense): dense mask of batches\n",
        "        EPS (float): epsilon to avoid nans\n",
        "\n",
        "    Returns:\n",
        "        adj: new adjacency = CTdist/vol(G)\n",
        "        loss: Cut Loss for CT Embedding (s)\n",
        "        ortho_loss: Loss regularization orthogonality in CT Embedding (s)\n",
        "    \"\"\"\n",
        "    x = x.unsqueeze(0) if x.dim() == 2 else x # adj torch.Size([b, N, f])\n",
        "    adj = adj.unsqueeze(0) if adj.dim() == 2 else adj # adj torch.Size([b, N, N]) \n",
        "    s = s.unsqueeze(0) if s.dim() == 2 else s # s torch.Size([b, N, k])\n",
        "    \n",
        "    s = torch.tanh(s) # torch.Size([20, N, k]) One k for each N of each graph\n",
        "    \n",
        "    # batck masking\n",
        "    (batch_size, num_nodes, _), k = x.size(), s.size(-1)\n",
        "    if mask is not None:\n",
        "        mask = mask.view(batch_size, num_nodes, 1).to(x.dtype)\n",
        "        x, s = x * mask, s * mask \n",
        "\n",
        "    # CT regularization\n",
        "    # Calculate degree d_flat and degree matrix d\n",
        "    d_flat = torch.einsum('ijk->ij', adj) # torch.Size([b, N]) \n",
        "    d = _rank3_diag(d_flat)+EPS  # d torch.Size([b, N, N])\n",
        "    \n",
        "    # Calculate CT_dist (distance matrix)\n",
        "    CT_dist = torch.cdist(s,s) # [20, N, k], [20, N, k]-> [20,N,N]\n",
        "\n",
        "    ## Calculate Vol (volumes): one per graph \n",
        "    vol = _rank3_trace(d) # torch.Size([20]) \n",
        "\n",
        "    ## Calculate out_adj as CT_dist/vol(G)\n",
        "    N = adj.size(1)\n",
        "    CT_dist = (CT_dist) / vol.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "    ## Mask with adjacency\n",
        "    adj = CT_dist*adj\n",
        "    \n",
        "    \n",
        "    # Losses\n",
        "    ## Calculate Laplacian L = D - A \n",
        "    L = d - adj\n",
        "    \n",
        "    ## Calculate out_adj as A_CT = S.T*L*S\n",
        "    out_adj = torch.matmul(torch.matmul(s.transpose(1, 2), L), s) #[b, k, N]*[b, N, N]-> [b, k ,N]*[b, N, k] = [20, k, k]\n",
        "    \n",
        "    ## Calculate CT_num \n",
        "    CT_num = _rank3_trace(out_adj) # mincut_num torch.Size([b]) one sum over each graph\n",
        "\n",
        "    ## Calculate CT_den \n",
        "    CT_den = _rank3_trace(\n",
        "        torch.matmul(torch.matmul(s.transpose(1, 2), d ), s))+EPS # [b, k, N]*[b, N, N]->[b, k, N]*[b, N, k] -> [b] one sum over each graph\n",
        "\n",
        "    CT_loss = CT_num / CT_den\n",
        "    CT_loss = torch.mean(CT_loss) # Mean over batch!\n",
        "    \n",
        "    ## Orthogonality regularization.\n",
        "    ss = torch.matmul(s.transpose(1, 2), s)  #[b, k, N]*[b, N, k]-> [b, k, k]\n",
        "\n",
        "    i_s = torch.eye(k).type_as(ss) # [k, k]\n",
        "    ortho_loss = torch.norm(\n",
        "        ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -\n",
        "        i_s)\n",
        "    ortho_loss = torch.mean(ortho_loss) # Mean over batch!\n",
        "    \n",
        "    return adj, CT_loss, ortho_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e51e58",
      "metadata": {
        "id": "86e51e58"
      },
      "source": [
        "**Use $\\mathtt{CT-Layer}$ for Graph Classification**\n",
        "<img src=\"https://github.com/ellisalicante/GraphRewiring-Tutorial/blob/main/figs/ctnetwork.png?raw=1\" alt=\"CT network\" style=\"width:300px;\"/> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GNN using CT-Layer"
      ],
      "metadata": {
        "id": "V2tdYX7qtm0A"
      },
      "id": "V2tdYX7qtm0A"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1f656163",
      "metadata": {
        "id": "1f656163"
      },
      "outputs": [],
      "source": [
        "class CTNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k_centers, hidden_channels=32, EPS=1e-15):\n",
        "        super(CTNet, self).__init__()\n",
        "        \n",
        "        self.EPS=EPS\n",
        "        #Message Passing Layers\n",
        "        self.conv1 = DenseGraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
        "        \n",
        "        # Pooling for CT embedding\n",
        "        num_of_centers1 =  k_centers # k1 #order of number of nodes\n",
        "        self.pool1 = Linear(hidden_channels, num_of_centers1)\n",
        "        \n",
        "        # Pooling for MinCut Layer\n",
        "        num_of_centers2 =  16 # k2 #mincut \n",
        "        self.pool2 = Linear(hidden_channels, num_of_centers2) \n",
        "\n",
        "        # MLPs towards out \n",
        "        self.lin1 = Linear(in_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin3 = Linear(hidden_channels, out_channels)\n",
        " \n",
        "\n",
        "    def forward(self, x, edge_index, batch):    # x torch.Size([N, N]),  data.batch  torch.Size([661])  \n",
        "        # Make all matrices dense\n",
        "        adj = to_dense_adj(edge_index, batch)   # adj torch.Size(B, N, N])\n",
        "        x, mask = to_dense_batch(x, batch)      \n",
        "\n",
        "        #First layer: linear MLP\n",
        "        x = self.lin1(x) \n",
        "        \n",
        "        if torch.isnan(adj).any():\n",
        "              print(\"adj nan\")\n",
        "        if torch.isnan(x).any():\n",
        "              print(\"x nan\")\n",
        "        \n",
        "        # CT REWIRING\n",
        "        s1  = self.pool1(x)\n",
        "        #adj = torch.Size([b, N, h]) --> CT Embedding\n",
        "        adj, CT_loss, ortho_loss1 = dense_CT_rewiring(x, adj, s1, mask, EPS = self.EPS) \n",
        "        #adj = torch.Size([b, N, N]) --> CT Distances\n",
        "        \n",
        "\n",
        "        # CONV1: Now on x and rewired adj: \n",
        "        x = self.conv1(x, adj) #out: x torch.Size([20, N, F'=32])\n",
        "\n",
        "        # MINCUT_POOL - Garph pooling\n",
        "        # MLP of k=16 outputs s\n",
        "        s2 = self.pool2(x) # s torch.Size([20, N, k])\n",
        "        \n",
        "        # Call to dense_cut_mincut_pool to get coarsened x, adj and the losses: k=16\n",
        "        x, adj, mincut_loss2, ortho_loss2 = dense_mincut_pool(x, adj, s2, mask, EPS=self.EPS) # out x torch.Size([20, k=16, F'=32]),  adj torch.Size([20, k2=16, k2=16])\n",
        "\n",
        "        # CONV2: Now on coarsened x and adj: \n",
        "        x = self.conv2(x, adj) #out x torch.Size([20, 16, 32])\n",
        "        \n",
        "        # Readout for each of the 20 graphs\n",
        "        x = x.sum(dim=1) \n",
        "        \n",
        "        # Final MLP for graph classification: hidden channels = 32\n",
        "        x = F.relu(self.lin2(x)) \n",
        "        x = self.lin3(x) \n",
        "        \n",
        "        #loss functions\n",
        "        CT_loss = CT_loss + ortho_loss1\n",
        "        mincut_loss = mincut_loss2 + ortho_loss2\n",
        "        \n",
        "        return F.log_softmax(x, dim=-1), CT_loss, mincut_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0a8d6f9f",
      "metadata": {
        "id": "0a8d6f9f"
      },
      "outputs": [],
      "source": [
        "def train(epoch, loader):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    correct = 0\n",
        "    #i = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out, mc_loss, o_loss = model(data.x, data.edge_index, data.batch) # data.batch  torch.Size([783])\n",
        "        loss = F.nll_loss(out, data.y.view(-1)) + mc_loss + o_loss\n",
        "        loss.backward()\n",
        "        loss_all += data.y.size(0) * loss.item()\n",
        "        optimizer.step()\n",
        "        correct += out.max(dim=1)[1].eq(data.y.view(-1)).sum().item() #accuracy in train AFTER EACH BACH\n",
        "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
        "        correct += pred.max(dim=1)[1].eq(data.y.view(-1)).sum().item()\n",
        "\n",
        "    return loss, correct / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Dataset and split\n",
        "\n",
        "**Change `batch_size` depending on the availability of you memory**"
      ],
      "metadata": {
        "id": "ykl_l7weqYjJ"
      },
      "id": "ykl_l7weqYjJ"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8451e83e",
      "metadata": {
        "id": "8451e83e"
      },
      "outputs": [],
      "source": [
        "#TRAIN_SPLIT = 1500\n",
        "#BATCH_SIZE = 16\n",
        "#num_of_centers = 100\n",
        "#dataset = TUDataset(root='data',name=\"REDDIT-BINARY\", transform = FeatureDegree())\n",
        "\n",
        "#TRAIN_SPLIT = 4500\n",
        "#BATCH_SIZE = 16\n",
        "#num_of_centers = 50\n",
        "#dataset = TUDataset(root='data',name=\"COLLAB\", transform = FeatureDegree())\n",
        "\n",
        "TRAIN_SPLIT = 800\n",
        "BATCH_SIZE = 16\n",
        "num_of_centers = 20\n",
        "dataset = TUDataset(root='data',name=\"IMDB-BINARY\", transform = FeatureDegree())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2287d420",
      "metadata": {
        "id": "2287d420"
      },
      "outputs": [],
      "source": [
        "seed = 12345\n",
        "torch.manual_seed(12345)\n",
        "stratified = True\n",
        "\n",
        "if stratified:\n",
        "  train_indices, test_indices = train_test_split(list(range(len(dataset.data.y))), test_size=0.15, \n",
        "                                                stratify=dataset.data.y,\n",
        "                                                random_state=seed, shuffle=True)\n",
        "\n",
        "  train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "  test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  \n",
        "else:\n",
        "  dataset = dataset.shuffle()\n",
        "  train_dataset = dataset[:TRAIN_SPLIT]\n",
        "  test_dataset = dataset[TRAIN_SPLIT:]\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create model and train"
      ],
      "metadata": {
        "id": "MdmyLDrNqobB"
      },
      "id": "MdmyLDrNqobB"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4965faca",
      "metadata": {
        "id": "4965faca"
      },
      "outputs": [],
      "source": [
        "EPS = 1e-15\n",
        "\n",
        "model = CTNet(dataset.num_features, dataset.num_classes, k_centers=num_of_centers, EPS=EPS).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b739a0cd",
      "metadata": {
        "id": "b739a0cd",
        "outputId": "5ebb139d-d7f3-4de8-efc5-d91f32c9f2c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.5059, Test Acc: 0.4600\n",
            "Epoch: 002, Train Acc: 0.5682, Test Acc: 0.4200\n",
            "Epoch: 003, Train Acc: 0.5906, Test Acc: 0.5467\n",
            "Epoch: 004, Train Acc: 0.6082, Test Acc: 0.5733\n",
            "Epoch: 005, Train Acc: 0.5871, Test Acc: 0.6400\n",
            "Epoch: 006, Train Acc: 0.6282, Test Acc: 0.6333\n",
            "Epoch: 007, Train Acc: 0.6306, Test Acc: 0.4733\n",
            "Epoch: 008, Train Acc: 0.6082, Test Acc: 0.4600\n",
            "Epoch: 009, Train Acc: 0.6341, Test Acc: 0.7267\n",
            "Epoch: 010, Train Acc: 0.6647, Test Acc: 0.7133\n",
            "Epoch: 011, Train Acc: 0.6224, Test Acc: 0.6933\n",
            "Epoch: 012, Train Acc: 0.6518, Test Acc: 0.6400\n",
            "Epoch: 013, Train Acc: 0.6576, Test Acc: 0.6667\n",
            "Epoch: 014, Train Acc: 0.6541, Test Acc: 0.6800\n",
            "Epoch: 015, Train Acc: 0.6482, Test Acc: 0.7333\n",
            "Epoch: 016, Train Acc: 0.6541, Test Acc: 0.7000\n",
            "Epoch: 017, Train Acc: 0.5941, Test Acc: 0.4733\n",
            "Epoch: 018, Train Acc: 0.6388, Test Acc: 0.7133\n",
            "Epoch: 019, Train Acc: 0.6259, Test Acc: 0.5333\n",
            "Epoch: 020, Train Acc: 0.5953, Test Acc: 0.5067\n",
            "Epoch: 021, Train Acc: 0.6118, Test Acc: 0.6600\n",
            "Epoch: 022, Train Acc: 0.6000, Test Acc: 0.4200\n",
            "Epoch: 023, Train Acc: 0.5847, Test Acc: 0.6867\n",
            "Epoch: 024, Train Acc: 0.6306, Test Acc: 0.5000\n",
            "Epoch: 025, Train Acc: 0.6024, Test Acc: 0.7000\n",
            "Epoch: 026, Train Acc: 0.6624, Test Acc: 0.7067\n",
            "Epoch: 027, Train Acc: 0.6459, Test Acc: 0.7200\n",
            "Epoch: 028, Train Acc: 0.6447, Test Acc: 0.6667\n",
            "Epoch: 029, Train Acc: 0.6518, Test Acc: 0.7200\n",
            "Epoch: 030, Train Acc: 0.6235, Test Acc: 0.6467\n",
            "Epoch: 031, Train Acc: 0.6212, Test Acc: 0.6867\n",
            "Epoch: 032, Train Acc: 0.6682, Test Acc: 0.7067\n",
            "Epoch: 033, Train Acc: 0.6612, Test Acc: 0.7267\n",
            "Epoch: 034, Train Acc: 0.6635, Test Acc: 0.6667\n",
            "Epoch: 035, Train Acc: 0.6353, Test Acc: 0.7200\n",
            "Epoch: 036, Train Acc: 0.6494, Test Acc: 0.6867\n",
            "Epoch: 037, Train Acc: 0.6765, Test Acc: 0.7400\n",
            "Epoch: 038, Train Acc: 0.6494, Test Acc: 0.7267\n",
            "Epoch: 039, Train Acc: 0.6753, Test Acc: 0.7200\n",
            "Epoch: 040, Train Acc: 0.6447, Test Acc: 0.7267\n",
            "Epoch: 041, Train Acc: 0.6482, Test Acc: 0.7000\n",
            "Epoch: 042, Train Acc: 0.6694, Test Acc: 0.6400\n",
            "Epoch: 043, Train Acc: 0.6718, Test Acc: 0.7200\n",
            "Epoch: 044, Train Acc: 0.6682, Test Acc: 0.7333\n",
            "Epoch: 045, Train Acc: 0.6718, Test Acc: 0.7133\n",
            "Epoch: 046, Train Acc: 0.6659, Test Acc: 0.7267\n",
            "Epoch: 047, Train Acc: 0.6906, Test Acc: 0.6267\n",
            "Epoch: 048, Train Acc: 0.6729, Test Acc: 0.7333\n",
            "Epoch: 049, Train Acc: 0.6824, Test Acc: 0.7267\n",
            "Epoch: 050, Train Acc: 0.6847, Test Acc: 0.6933\n",
            "Epoch: 051, Train Acc: 0.6718, Test Acc: 0.6467\n",
            "Epoch: 052, Train Acc: 0.6753, Test Acc: 0.7333\n",
            "Epoch: 053, Train Acc: 0.6682, Test Acc: 0.7400\n",
            "Epoch: 054, Train Acc: 0.6835, Test Acc: 0.6800\n",
            "Epoch: 055, Train Acc: 0.6847, Test Acc: 0.6867\n",
            "Epoch: 056, Train Acc: 0.6835, Test Acc: 0.6600\n",
            "Epoch: 057, Train Acc: 0.6518, Test Acc: 0.7200\n",
            "Epoch: 058, Train Acc: 0.6835, Test Acc: 0.6600\n",
            "Epoch: 059, Train Acc: 0.6682, Test Acc: 0.6933\n",
            "Epoch: 060, Train Acc: 0.6965, Test Acc: 0.6733\n",
            "Epoch: 061, Train Acc: 0.6682, Test Acc: 0.7667\n",
            "Epoch: 062, Train Acc: 0.6918, Test Acc: 0.7267\n",
            "Epoch: 063, Train Acc: 0.6729, Test Acc: 0.7600\n",
            "Epoch: 064, Train Acc: 0.6988, Test Acc: 0.6933\n",
            "Epoch: 065, Train Acc: 0.6859, Test Acc: 0.7533\n",
            "Epoch: 066, Train Acc: 0.6976, Test Acc: 0.7467\n",
            "Epoch: 067, Train Acc: 0.7012, Test Acc: 0.7400\n",
            "Epoch: 068, Train Acc: 0.6776, Test Acc: 0.7333\n",
            "Epoch: 069, Train Acc: 0.6694, Test Acc: 0.7333\n",
            "Epoch: 070, Train Acc: 0.6976, Test Acc: 0.6867\n",
            "Epoch: 071, Train Acc: 0.6765, Test Acc: 0.7467\n",
            "Epoch: 072, Train Acc: 0.7024, Test Acc: 0.7333\n",
            "Epoch: 073, Train Acc: 0.6800, Test Acc: 0.7467\n",
            "Epoch: 074, Train Acc: 0.6776, Test Acc: 0.7400\n",
            "Epoch: 075, Train Acc: 0.7035, Test Acc: 0.7533\n",
            "Epoch: 076, Train Acc: 0.7094, Test Acc: 0.6333\n",
            "Epoch: 077, Train Acc: 0.6953, Test Acc: 0.6600\n",
            "Epoch: 078, Train Acc: 0.6976, Test Acc: 0.7467\n",
            "Epoch: 079, Train Acc: 0.6929, Test Acc: 0.7067\n",
            "Epoch: 080, Train Acc: 0.6706, Test Acc: 0.6533\n",
            "Epoch: 081, Train Acc: 0.6859, Test Acc: 0.7467\n",
            "Epoch: 082, Train Acc: 0.6859, Test Acc: 0.7333\n",
            "Epoch: 083, Train Acc: 0.7035, Test Acc: 0.7200\n",
            "Epoch: 084, Train Acc: 0.7012, Test Acc: 0.7400\n",
            "Epoch: 085, Train Acc: 0.7000, Test Acc: 0.7533\n",
            "Epoch: 086, Train Acc: 0.6847, Test Acc: 0.7333\n",
            "Epoch: 087, Train Acc: 0.6988, Test Acc: 0.7600\n",
            "Epoch: 088, Train Acc: 0.7047, Test Acc: 0.7600\n",
            "Epoch: 089, Train Acc: 0.6965, Test Acc: 0.7533\n",
            "Epoch: 090, Train Acc: 0.7047, Test Acc: 0.7333\n",
            "Epoch: 091, Train Acc: 0.6965, Test Acc: 0.7533\n",
            "Epoch: 092, Train Acc: 0.6788, Test Acc: 0.7667\n",
            "Epoch: 093, Train Acc: 0.6753, Test Acc: 0.7533\n",
            "Epoch: 094, Train Acc: 0.6953, Test Acc: 0.7333\n",
            "Epoch: 095, Train Acc: 0.6859, Test Acc: 0.6867\n",
            "Epoch: 096, Train Acc: 0.6906, Test Acc: 0.7533\n",
            "Epoch: 097, Train Acc: 0.6729, Test Acc: 0.7467\n",
            "Epoch: 098, Train Acc: 0.7024, Test Acc: 0.6867\n",
            "Epoch: 099, Train Acc: 0.7012, Test Acc: 0.7400\n"
          ]
        }
      ],
      "source": [
        "optimizer.zero_grad()\n",
        "for epoch in range(1, 100):\n",
        "    train_loss, train_acc = train(epoch, train_loader)\n",
        "    #_, train_acc = test(train_loader)\n",
        "    _, test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be21f4e7",
      "metadata": {
        "id": "be21f4e7"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHOUKtUdrr-X"
      },
      "id": "zHOUKtUdrr-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVP7Ssc_rr5Q"
      },
      "id": "BVP7Ssc_rr5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srdQ_31prrzY"
      },
      "id": "srdQ_31prrzY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbAIJRfHrrxJ"
      },
      "id": "zbAIJRfHrrxJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting CT Layer: CT Distances and CT Embedding"
      ],
      "metadata": {
        "id": "3MKGLZMZrsZB"
      },
      "id": "3MKGLZMZrsZB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8CPDmxDrru5"
      },
      "id": "d8CPDmxDrru5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1954040a",
      "metadata": {
        "id": "1954040a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1143d0",
      "metadata": {
        "id": "6d1143d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd98d311",
      "metadata": {
        "id": "fd98d311"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "edf400a8",
      "metadata": {
        "id": "edf400a8"
      },
      "source": [
        "## Node classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad78213",
      "metadata": {
        "id": "1ad78213"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "975064d8d3d76841da6545d74f7ce08046b8a22628138d3f8f012e5fa8e266fc"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}